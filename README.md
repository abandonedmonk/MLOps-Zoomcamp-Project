# Heart Disease Prediction (MLOps Pipeline)

A complete MLOps project for heart disease prediction using machine learning. This project focuses on building an end-to-end pipeline with experiment tracking, orchestration, deployment, and containerization for learning and personal development.


# Overview

This project implements a production-grade MLOps pipeline for predicting heart disease using the UCI Heart Disease dataset (Cleveland, 303 instances, 13 features). The pipeline leverages MLflow for experiment tracking, Prefect for workflow orchestration, FastAPI for model serving, Docker for containerization. Dependencies are managed using Poetry for reproducibility. The goal is to deliver a scalable, automated, and interpretable machine learning system for healthcare applications.

# Problem Statement

The project aims to predict the presence of heart disease (binary classification) using clinical features like age, cholesterol, and chest pain type. Key objectives include achieving high model accuracy (≥85%).

# Dataset

The UCI Heart Disease dataset (Cleveland) contains 303 instances with 13 features (e.g., age, sex, cholesterol, chest pain type) and a target variable indicating heart disease presence (0 = no, 1-4 = yes, simplified to binary). The dataset is stored in data/raw/cleveland.data.

* Download the Dataset from: https://archive.ics.uci.edu/dataset/45/heart+disease

## 📦 Project Features

* ✅ Structured with **Cookiecutter Data Science** layout
* ✅ Models: `LogisticRegression`, `DecisionTree`, `RandomForest`, `HistGradientBoosting`, `XGBoost`
* ✅ Preprocessing with `ColumnTransformer`
* ✅ Experiment tracking using **MLflow** (local DB)
* ✅ Orchestration with **Prefect 2.x Cloud**
* ✅ Packaged and **Dockerized FastAPI** for inference
* ✅ Automatic download of the **best model** + preprocessor
* ✅ `.env` secrets loading for secure config (not used)



## 📁 Folder Structure

```
├── LICENSE                    <- License for open-source distribution (e.g., MIT, Apache)
├── Makefile                   <- Convenience commands like `make train`, `make lint`, etc.
├── README.md                  <- Top-level project documentation with setup and usage instructions
│
├── Pipfile                    <- Dependency management (Pipenv)
├── pyproject.toml             <- Project config for black, isort, pytest, etc.
├── requirements.txt           <- Python dependencies for reproducibility
├── setup.cfg                  <- Tool-specific configuration (e.g., flake8, mypy)
│
├── api                        <- Production-ready REST API using FastAPI
│   ├── Dockerfile             <- Docker container definition for the API
│   ├── main.py                <- Main FastAPI app and routes
│   ├── requirements.txt       <- Dependencies specific to API container
│   └── schema.py              <- Pydantic schema for input validation
│
├── data                       <- All project data
│   ├── external               <- Third-party sourced data
│   ├── interim                <- Intermediate data (after cleaning, before modeling)
│   ├── processed              <- Final data used for modeling
│   │   └── processed_cleveland_data.csv
│   └── raw                    <- Raw, original, and immutable data dumps
│       ├── processed.cleveland.data
│       ├── processed.hungarian.data
│       └── processed.switzerland.data
│
├── heart_disease_prediction   <- Core source code and pipeline logic
│   ├── __init__.py            <- Package initializer
│   ├── data.py                <- Data loading and preprocessing logic
│   ├── load_model.py          <- Loads best model + preprocessor from MLflow
│   ├── prefect_flow.py        <- Prefect 2.x orchestration entrypoint
│   ├── prefect.yaml           <- Prefect deployment config (autogenerated or manual)
│   ├── register.py            <- Registers best-performing model to MLflow
│   └── train.py               <- Trains multiple ML models and logs them to MLflow
│
├── models                     <- Persisted and serialized ML models
│   ├── model.bin              <- Saved model artifact (for download or reuse)
│   └── pipeline.pkl           <- Combined preprocessor + model object
│
├── notebooks                  <- Exploratory and experiment notebooks
│   └── heart_disease_experiment.ipynb
│
├── tests                      <- Unit and integration tests
│   ├── test_api.py            <- Tests for FastAPI endpoints
│   └── test_data.py           <- Tests for data processing and loading

```

## 🔧 Setup Instructions

### 1. ✅ Create and Activate Environment

```bash
# Using pip + virtualenv
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### 2. ✅ Run MLflow Tracking UI (Local)

```bash
mkdir mlruns
mlflow ui --backend-store-uri sqlite:///mlruns/mlflow.db --port 5000
```

Access: [http://localhost:5000](http://localhost:5000)

---

### 3. ✅ Run Full Pipeline via Prefect Cloud

* Make an account in Prefect login to their cloud platform.
* Create a Workflow and name it *heart-disease*, if you chose to name it something else then change the command given below.
* Make sure you’ve deployed using:

```bash
prefect deploy prefect_flow.py:full_pipeline --name heart-disease --work-queue default --cron "0 0 * * 0"
```
This command will set the job to automatically run once every week (on Sunday)

To run this directly and get the result:
```bash
cd heart_disease_prediction
python prefect_flow.py
```
After which log into the MLFlow UI to download the latest model in the Artifact Registry

---

### 4. ✅ Training & Registration

* `train.py` → trains and logs models
* `register.py` → picks best model by accuracy and registers
* `load_model.py` → loads and downloads best model + preprocessor

Models are saved to: `models/pipeline.pkl`

They run together in the orchestration step when `prefect_flow.py` is invoked

---

### 5. ✅ FastAPI Inference Server

```bash
uvicorn api.main:app --reload
```

POST JSON at: `http://127.0.0.1:8000/predict`

**Test Client Example**:

```bash
cd tests
python test_api.py
```

* NOTE: The above values can be changed but in accordance with the datatypes mentioned in the official documentation of the dataset

---

### 6. 🐳 Dockerized API

```bash
docker build -t heart-api .
docker run -p 8000:8000 heart-api
```
**Testing (same as above)**
```bash
cd tests
python test_api.py
```


## ✅ What's Done

* [x] Data preprocessing and transformation
* [x] Model training with multiple algorithms
* [x] MLflow experiment logging
* [x] Model registration and download
* [x] Prefect orchestration on Cloud
* [x] FastAPI deployment for inference
* [x] Docker containerization
* [x] `.env`-based config loading


## 📌 Future Additions

* [ ] Deploy MLflow tracking to AWS S3/RDS
* [ ] SageMaker model deployment
* [ ] Evidently for monitoring drift in production
* [ ] CI/CD integration (GitHub Actions)
* [ ] Unit Test Code


## 🙌 Credits

This project was built as part of the **MLOps Zoomcamp**, with custom extensions for full orchestration and deployment.

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

---